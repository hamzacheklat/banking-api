Parfait ğŸ‘
Je te donne **LE CODE COMPLET, COHÃ‰RENT, PRÃŠT Ã€ Lâ€™EMPLOI**, pour un **RAG multi-livres Python**, avec :

âœ… Lecture PDF
âœ… Extraction **Table des MatiÃ¨res (chapitres / sections)**
âœ… Extraction texte **par section rÃ©elle**
âœ… Conversion **PDF â†’ Markdown structurÃ©**
âœ… Chunking **1 section = 1 chunk (sans max_chars)**
âœ… Vectorisation FAISS
âœ… PrÃªt pour brancher un LLM derriÃ¨re

Je te mets **TOUS les fichiers**, un par un, exactement comme dans lâ€™arborescence.

---

# ğŸ“ Arborescence finale

```
rag/
â”œâ”€â”€ ingest/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ toc_extractor.py
â”‚   â”œâ”€â”€ pdf_loader.py
â”‚   â”œâ”€â”€ pdf_to_markdown.py
â”‚   â”œâ”€â”€ section_chunker.py
â”‚   â””â”€â”€ ingest_pipeline.py
â”‚
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ embedder.py
â”‚   â””â”€â”€ vector_store.py
â”‚
â”œâ”€â”€ retriever/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ search.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_pdfs/
â”‚   â””â”€â”€ chunks/
â”‚
â”œâ”€â”€ main.py
â””â”€â”€ requirements.txt
```

---

# ğŸ“¦ requirements.txt

```
pymupdf
sentence-transformers
faiss-cpu
numpy
```

---

# ğŸ“˜ ingest/toc_extractor.py

ğŸ‘‰ Extraction **Table des MatiÃ¨res rÃ©elle**

```python
import fitz  # PyMuPDF


def extract_toc(pdf_path: str):
    doc = fitz.open(pdf_path)
    toc = doc.get_toc(simple=True)

    if not toc:
        raise ValueError("No Table of Contents found in PDF")

    sections = []

    for i, (level, title, page) in enumerate(toc):
        start_page = page - 1
        end_page = (
            toc[i + 1][2] - 2
            if i + 1 < len(toc)
            else doc.page_count - 1
        )

        sections.append({
            "level": level,
            "title": title.strip(),
            "start_page": start_page,
            "end_page": end_page
        })

    return sections
```

---

# ğŸ“„ ingest/pdf_loader.py

ğŸ‘‰ Extraction texte **par section**

```python
import fitz


def extract_section_text(pdf_path: str, section: dict) -> str:
    doc = fitz.open(pdf_path)
    texts = []

    for page_num in range(section["start_page"], section["end_page"] + 1):
        page = doc[page_num]
        texts.append(page.get_text("text"))

    return "\n".join(texts).strip()
```

---

# ğŸ§¾ ingest/pdf_to_markdown.py

ğŸ‘‰ Conversion texte â†’ Markdown propre

```python
import re


def to_markdown(title: str, text: str) -> str:
    md = f"# {title}\n\n"

    for line in text.splitlines():
        line = line.strip()

        if not line:
            md += "\n"
            continue

        # Detect numbered titles
        if re.match(r"^\d+(\.\d+)*\s+", line):
            md += f"## {line}\n"
        else:
            md += line + "\n"

    return md.strip()
```

---

# âœ‚ï¸ ingest/section_chunker.py

ğŸ‘‰ **1 section = 1 chunk (ce que tu veux)**

```python
def chunk_by_section(book: str, section: dict, markdown: str):
    return [{
        "book": book,
        "section": section["title"],
        "level": section["level"],
        "text": markdown,
        "metadata": {
            "start_page": section["start_page"],
            "end_page": section["end_page"]
        }
    }]
```

---

# ğŸ”„ ingest/ingest_pipeline.py

ğŸ‘‰ Pipeline complet PDF â†’ chunks

```python
from ingest.toc_extractor import extract_toc
from ingest.pdf_loader import extract_section_text
from ingest.pdf_to_markdown import to_markdown
from ingest.section_chunker import chunk_by_section


def ingest_book(pdf_path: str, book_name: str):
    sections = extract_toc(pdf_path)
    chunks = []

    for section in sections:
        raw_text = extract_section_text(pdf_path, section)

        if not raw_text.strip():
            continue

        markdown = to_markdown(section["title"], raw_text)

        chunks.extend(
            chunk_by_section(book_name, section, markdown)
        )

    return chunks
```

---

# ğŸ§  embeddings/embedder.py

ğŸ‘‰ Embeddings modernes (E5)

```python
from sentence_transformers import SentenceTransformer

_model = SentenceTransformer("intfloat/e5-large-v2")


def embed_texts(texts: list[str]):
    return _model.encode(
        texts,
        normalize_embeddings=True,
        show_progress_bar=True
    )
```

---

# ğŸ“¦ embeddings/vector_store.py

ğŸ‘‰ FAISS simple et efficace

```python
import faiss
import numpy as np


class VectorStore:
    def __init__(self, dim: int):
        self.index = faiss.IndexFlatIP(dim)
        self.documents = []

    def add(self, embeddings, docs):
        self.index.add(np.array(embeddings).astype("float32"))
        self.documents.extend(docs)

    def search(self, query_embedding, k=5):
        scores, ids = self.index.search(
            np.array([query_embedding]).astype("float32"),
            k
        )
        return [self.documents[i] for i in ids[0]]
```

---

# ğŸ” retriever/search.py

ğŸ‘‰ Recherche sÃ©mantique

```python
from embeddings.embedder import embed_texts


def semantic_search(store, query: str, k=5):
    query_emb = embed_texts([query])[0]
    return store.search(query_emb, k)
```

---

# ğŸš€ main.py

ğŸ‘‰ Multi-livres, prÃªt prod

```python
from ingest.ingest_pipeline import ingest_book
from embeddings.embedder import embed_texts
from embeddings.vector_store import VectorStore
from retriever.search import semantic_search


BOOKS = {
    "oracle_performance": "data/raw_pdfs/oracle_perf.pdf",
    "postgresql": "data/raw_pdfs/postgres.pdf"
}


def main():
    all_chunks = []

    for book, path in BOOKS.items():
        print(f"Ingesting {book}...")
        chunks = ingest_book(path, book)
        all_chunks.extend(chunks)

    texts = [c["text"] for c in all_chunks]
    embeddings = embed_texts(texts)

    store = VectorStore(dim=len(embeddings[0]))
    store.add(embeddings, all_chunks)

    # TEST
    results = semantic_search(
        store,
        "explain buffer cache contention",
        k=3
    )

    for r in results:
        print("\n---")
        print(r["book"], "->", r["section"])
        print(r["text"][:800])


if __name__ == "__main__":
    main()
```

---

# âœ… Ce que tu as maintenant

âœ”ï¸ Chunking **100% sÃ©mantique**
âœ”ï¸ Chapitres rÃ©els (TOC)
âœ”ï¸ Multi-livres
âœ”ï¸ Aucune limite `max_chars`
âœ”ï¸ Contexte LLM propre
âœ”ï¸ Architecture claire & extensible

---

## ğŸ”¥ Prochaines amÃ©liorations (je peux te les coder direct)

* Split automatique **par sous-sections Markdown**
* Fallback si le PDF **nâ€™a pas de TOC**
* Docling / Nougat pour Markdown parfait
* Reranking cross-encoder
* Prompt RAG Oracle / DBA ultra-strict (comme tu aimes)

Dis-moi ğŸ‘‰ **quelle amÃ©lioration tu veux en premier** et je te donne le code direct.
