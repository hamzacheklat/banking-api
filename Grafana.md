Parfait â€” ton setup est dÃ©jÃ  trÃ¨s solide ğŸ‘
On va maintenant **le rendre production-grade** :

âœ… alertes prÃªtes Ã  dÃ©ployer
âœ… persistance Prometheus (backup + restart safe)
âœ… dashboard Grafana JSON
âœ… alert rules JSON Grafana

Je te donne **du concret prÃªt Ã  coller**.

---

# âœ… 1ï¸âƒ£ Sauvegarde + persistance Prometheus (critique en prod)

Tu as dÃ©jÃ  un PVC â€” bien ğŸ‘ â€” mais Ã§a protÃ¨ge **uniquement contre un restart Pod**, pas :

* corruption disque
* suppression PVC
* cluster crash

ğŸ‘‰ On ajoute **snapshot automatique Prometheus**.

---

## ğŸ”¹ Activer lâ€™API snapshot Prometheus

Dans le deployment :

```yaml
args:
  - "--storage.tsdb.path=/prometheus"
  - "--storage.tsdb.retention.time=15d"
  - "--web.enable-admin-api"
```

---

## ğŸ”¹ CronJob Kubernetes â€” snapshot automatique

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: prometheus-backup
spec:
  schedule: "0 */6 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: snapshot
            image: curlimages/curl
            command:
            - sh
            - -c
            - |
              curl -X POST http://prometheus:9090/api/v1/admin/tsdb/snapshot
          restartPolicy: OnFailure
```

ğŸ‘‰ Snapshot toutes les 6h.

---

## ğŸ”¹ (Option recommandÃ© prod)

Monter un volume externe :

```
/prometheus/snapshots â†’ S3 / NFS / backup system
```

---

# ğŸš¨ 2ï¸âƒ£ Alertes Prometheus (rules YAML)

ğŸ‘‰ Ã€ mettre dans un ConfigMap `alerts.yaml`.

---

## ğŸ”¥ API DOWN

```yaml
groups:
- name: api-health
  rules:

  - alert: APIDown
    expr: up == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "API down"
      description: "An API endpoint is unreachable"
```

---

## ğŸ”¥ Trop dâ€™erreurs

```yaml
  - alert: HighErrorRate
    expr: |
      (
        sum(rate(http_requests_total{status=~"5.."}[5m]))
        /
        sum(rate(http_requests_total[5m]))
      ) > 0.05
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High error rate"
      description: "More than 5% errors"
```

---

## ğŸ”¥ API lente

```yaml
  - alert: HighLatency
    expr: |
      histogram_quantile(
        0.95,
        sum by (le) (
          rate(http_request_duration_seconds_bucket[5m])
        )
      ) > 2
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "API latency high"
      description: "P95 latency > 2s"
```

---

## ğŸ”¥ Saturation (in flight)

```yaml
  - alert: HighInflightRequests
    expr: http_requests_in_flight > 100
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "Too many in-flight requests"
```

---

# ğŸ“Š 3ï¸âƒ£ Dashboard Grafana â€” JSON prÃªt Ã  importer

ğŸ‘‰ Import â†’ JSON â†’ coller ceci.

{
"title": "API Overview",
"schemaVersion": 39,
"panels": [
{
"type": "timeseries",
"title": "Requests/sec",
"targets": [
{
"expr": "sum(rate(http_requests_total[1m]))"
}
]
},
{
"type": "timeseries",
"title": "5xx Errors/sec",
"targets": [
{
"expr": "sum(rate(http_requests_total{status=~"5.."}[1m]))"
}
]
},
{
"type": "timeseries",
"title": "Error Rate %",
"targets": [
{
"expr": "(sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m]))) * 100"
}
]
},
{
"type": "timeseries",
"title": "Latency P95",
"targets": [
{
"expr": "histogram_quantile(0.95, sum by (le) (rate(http_request_duration_seconds_bucket[5m])))"
}
]
},
{
"type": "timeseries",
"title": "Requests in Flight",
"targets": [
{
"expr": "http_requests_in_flight"
}
]
}
]
}

---

# ğŸš¨ 4ï¸âƒ£ Alertes Grafana â€” JSON provisioning

ğŸ‘‰ Ã€ placer dans `/etc/grafana/provisioning/alerting/api-alerts.json`

{
"apiVersion": 1,
"groups": [
{
"name": "API Alerts",
"folder": "API",
"interval": "1m",
"rules": [
{
"title": "High Error Rate",
"condition": "A",
"data": [
{
"refId": "A",
"expr": "(sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m]))) > 0.05"
}
]
},
{
"title": "API Down",
"condition": "A",
"data": [
{
"refId": "A",
"expr": "up == 0"
}
]
},
{
"title": "High Latency",
"condition": "A",
"data": [
{
"refId": "A",
"expr": "histogram_quantile(0.95, sum by (le) (rate(http_request_duration_seconds_bucket[5m]))) > 2"
}
]
}
]
}
]
}

---

# âœ… RÃ©sultat final â€” architecture robuste

```
API metrics
    â†“
Prometheus (PVC + snapshots)
    â†“
Grafana dashboards + alerting email
```

âœ” restart safe
âœ” sauvegardes
âœ” alertes prod
âœ” dashboard prÃªt

---

Si tu veux, je peux te faire ensuite :

ğŸ‘‰ Alertmanager config email Slack
ğŸ‘‰ Dashboard par endpoint
ğŸ‘‰ SLO/SLA monitoring
ğŸ‘‰ autoscaling basÃ© sur metrics
ğŸ‘‰ federation multi clusters

Dis-moi ce que tu veux next.
