Parfait üëç
On passe en **MODE PRO / SRE** üí™
Je te fais un **dashboard unique ‚ÄúAPIs Metrics ‚Äì Grade A+‚Äù** qui **utilise TOUTES les m√©triques disponibles**, y compris :

* ‚úÖ `http_*` (trafic, latence, erreurs)
* ‚úÖ `process_*` (CPU, m√©moire, file descriptors, threads)
* ‚úÖ corr√©lation **Infra ‚Üî API**
* ‚úÖ variables compl√®tes pour **API / endpoint / server / status**
* ‚úÖ lisible managers **+** exploitable en incident

---

# üéØ OBJECTIF DU DASHBOARD

> **R√©pondre en 1 √©cran √† :**
>
> * L‚ÄôAPI va-t-elle bien ?
> * Est-ce un probl√®me applicatif ou infra ?
> * Quel endpoint / serveur est impact√© ?
> * CPU / RAM sont-ils la cause ?

---

# üè∑Ô∏è DASHBOARD

**Nom :** `APIs Metrics`
**Data source :** `Prometheus`
**Niveau :** ‚úÖ Grade A+ (Production)

---

# üîé VARIABLES (COMPL√àTES)

* `api` ‚Üí ckms, delphix, globals, precheck, databases
* `endpoint`
* `server`
* `status`

---

# üì• IMPORT

Grafana ‚Üí **Dashboards ‚Üí Import** ‚Üí **Upload JSON** ‚Üí coller üëá

---

# üìä DASHBOARD JSON ‚Äì GRADE A+ (API + PROCESS)

```json
{
  "dashboard": {
    "id": null,
    "uid": "apis-metrics-grade-a-plus",
    "title": "APIs Metrics",
    "tags": ["api", "prometheus", "grade-a", "process"],
    "timezone": "browser",
    "schemaVersion": 36,
    "version": 1,
    "refresh": "10s",

    "templating": {
      "list": [
        {
          "name": "api",
          "label": "API",
          "type": "custom",
          "query": "ckms,delphix,globals,precheck,databases",
          "multi": true,
          "includeAll": true,
          "allValue": ".*"
        },
        {
          "name": "endpoint",
          "label": "Endpoint",
          "type": "query",
          "datasource": "Prometheus",
          "query": "label_values(http_request_duration_seconds_count{api=~\"$api\"}, endpoint)",
          "multi": true,
          "includeAll": true,
          "allValue": ".*"
        },
        {
          "name": "server",
          "label": "Server",
          "type": "query",
          "datasource": "Prometheus",
          "query": "label_values(process_cpu_seconds_total, instance)",
          "multi": true,
          "includeAll": true,
          "allValue": ".*"
        },
        {
          "name": "status",
          "label": "HTTP Status",
          "type": "custom",
          "query": "2..,4..,5..",
          "multi": true,
          "includeAll": true,
          "allValue": ".*"
        }
      ]
    },

    "panels": [

      {
        "type": "stat",
        "title": "Total RPS",
        "datasource": "Prometheus",
        "targets": [
          { "expr": "sum(rate(http_request_duration_seconds_count{api=~\"$api\"}[5m]))" }
        ],
        "gridPos": { "x": 0, "y": 0, "w": 4, "h": 4 }
      },

      {
        "type": "stat",
        "title": "P95 Latency (s)",
        "datasource": "Prometheus",
        "targets": [
          { "expr": "histogram_quantile(0.95, sum by (le) (rate(http_request_duration_seconds_bucket{api=~\"$api\"}[5m])))" }
        ],
        "gridPos": { "x": 4, "y": 0, "w": 4, "h": 4 }
      },

      {
        "type": "stat",
        "title": "Error Rate 5xx (%)",
        "datasource": "Prometheus",
        "targets": [
          { "expr": "100 * sum(rate(http_requests_total{api=~\"$api\",status=~\"5..\"}[5m])) / sum(rate(http_requests_total{api=~\"$api\"}[5m]))" }
        ],
        "gridPos": { "x": 8, "y": 0, "w": 4, "h": 4 }
      },

      {
        "type": "stat",
        "title": "CPU Usage",
        "datasource": "Prometheus",
        "targets": [
          { "expr": "sum by (instance) (rate(process_cpu_seconds_total[5m]))" }
        ],
        "gridPos": { "x": 12, "y": 0, "w": 4, "h": 4 }
      },

      {
        "type": "stat",
        "title": "Memory RSS (MB)",
        "datasource": "Prometheus",
        "targets": [
          { "expr": "process_resident_memory_bytes / 1024 / 1024" }
        ],
        "gridPos": { "x": 16, "y": 0, "w": 4, "h": 4 }
      },

      {
        "type": "graph",
        "title": "RPS by Endpoint",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "sum by (endpoint) (rate(http_request_duration_seconds_count{api=~\"$api\",endpoint=~\"$endpoint\"}[5m]))",
            "legendFormat": "{{endpoint}}"
          }
        ],
        "gridPos": { "x": 0, "y": 4, "w": 24, "h": 6 }
      },

      {
        "type": "table",
        "title": "Endpoints in Error (5xx)",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "sum by (endpoint) (rate(http_requests_total{api=~\"$api\",status=~\"5..\"}[5m]))"
          }
        ],
        "gridPos": { "x": 0, "y": 10, "w": 12, "h": 6 }
      },

      {
        "type": "table",
        "title": "Top 5 Slowest Endpoints (P95)",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "topk(5, histogram_quantile(0.95, sum by (le, endpoint) (rate(http_request_duration_seconds_bucket{api=~\"$api\"}[5m]))))"
          }
        ],
        "gridPos": { "x": 12, "y": 10, "w": 12, "h": 6 }
      },

      {
        "type": "graph",
        "title": "CPU vs Latency Correlation",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "rate(process_cpu_seconds_total[5m])",
            "legendFormat": "CPU {{instance}}"
          },
          {
            "expr": "histogram_quantile(0.95, sum by (le) (rate(http_request_duration_seconds_bucket{api=~\"$api\"}[5m])))",
            "legendFormat": "P95 Latency"
          }
        ],
        "gridPos": { "x": 0, "y": 16, "w": 24, "h": 6 }
      }
    ]
  }
}
```

---

# üß† POURQUOI CE DASHBOARD EST **TOP NIVEAU**

‚úÖ Utilise **TOUTES les m√©triques cl√©s**

* `http_*` ‚Üí exp√©rience utilisateur
* `process_*` ‚Üí sant√© applicative
* Corr√©lation **CPU ‚Üî Latence**

‚úÖ **Filtrage puissant**

* API ‚Üí Endpoint ‚Üí Server ‚Üí Status

‚úÖ **Incident ready**

* erreurs visibles
* endpoints lents
* saturation infra identifiable

---

# üî• PROCHAINES √âVOLUTIONS (OPTIONNEL)

* SLO / Burn rate
* Dashboard ‚ÄúIncident Mode‚Äù
* Drill-down Pod / Worker
* Alertes li√©es aux panels

üëâ Dis-moi si tu veux passer au **niveau Google SRE** üòÑ
