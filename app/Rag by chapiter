Parfait. On va faire **le pipeline complet, propre, industrialisable**, exactement adaptÃ© Ã  **ce livre Oracle 12c** et Ã  un **RAG sÃ©rieux**.
Je te donne **lâ€™architecture + le code clÃ©**, sans bullshit, avec des dÃ©cisions expliquÃ©es.

---

# ğŸ—ï¸ PIPELINE COMPLET â€” PDF â†’ RAG VECTOR (PROD-READY)

## ğŸ¯ RÃ©sultat final

Tu obtiens :

* un **Markdown structurÃ©** fidÃ¨le au livre
* des **chunks sÃ©mantiques stables** (par section 1-x)
* des **embeddings propres**
* stockÃ©s dans **Oracle VECTOR**
* prÃªts pour un **RAG prÃ©cis, non verbeux**

---

# ğŸ§  Architecture globale

```
PDF
 â””â”€â–¶ Extraction texte (pdfplumber)
     â””â”€â–¶ Normalisation
         â””â”€â–¶ DÃ©tection chapitres
             â””â”€â–¶ DÃ©tection sections 1-x
                 â””â”€â–¶ Split Problem / Solution / How It Works
                     â””â”€â–¶ Markdown structurÃ©
                         â””â”€â–¶ Chunking intelligent
                             â””â”€â–¶ Embeddings
                                 â””â”€â–¶ Oracle Vector DB
```

---

# ğŸ“¦ DÃ©pendances

```bash
pip install pdfplumber tiktoken openai oracledb
```

---

# âš™ï¸ CONFIG GLOBALE

```python
PDF_PATH = "oracle_12c_tuning.pdf"

EMBED_MODEL = "text-embedding-3-large"
MAX_CHUNK_TOKENS = 1200

ORACLE_DSN = "localhost:1521/XEPDB1"
ORACLE_USER = "rag"
ORACLE_PWD = "rag"
```

---

# 1ï¸âƒ£ PDF â†’ TEXTE

```python
import pdfplumber

def pdf_to_pages(path):
    pages = []
    with pdfplumber.open(path) as pdf:
        for i, page in enumerate(pdf.pages):
            text = page.extract_text()
            if text:
                pages.append({
                    "page": i + 1,
                    "text": text
                })
    return pages
```

---

# 2ï¸âƒ£ NORMALISATION

Le PDF contient :

* sauts de ligne cassÃ©s
* en-tÃªtes rÃ©pÃ©tÃ©es
* URLs parasites

```python
import re

def normalize(text):
    text = re.sub(r"www\.it-ebooks\.info", "", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()
```

---

# 3ï¸âƒ£ CHAPITRES

```python
CHAPTER_RE = re.compile(
    r"CHAPTER\s+(\d+)\s*\n([A-Z][A-Za-z\s]+)",
    re.MULTILINE
)

def extract_chapters(full_text):
    chapters = []
    matches = list(CHAPTER_RE.finditer(full_text))

    for i, m in enumerate(matches):
        start = m.end()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(full_text)

        chapters.append({
            "number": m.group(1),
            "title": m.group(2).strip(),
            "text": full_text[start:end]
        })
    return chapters
```

---

# 4ï¸âƒ£ SECTIONS 1-x (RECIPES)

```python
SECTION_RE = re.compile(
    r"(\d+-\d+)\.\s+(.+)",
    re.MULTILINE
)

def extract_sections(chapter):
    sections = []
    matches = list(SECTION_RE.finditer(chapter["text"]))

    for i, m in enumerate(matches):
        start = m.end()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(chapter["text"])

        sections.append({
            "chapter": chapter["number"],
            "chapter_title": chapter["title"],
            "section_id": m.group(1),
            "title": m.group(2).strip(),
            "text": chapter["text"][start:end]
        })
    return sections
```

---

# 5ï¸âƒ£ SOUS-SECTIONS (Problem / Solution / How It Works)

```python
SUB_RE = re.compile(r"(Problem|Solution|How It Works)")

def split_subsections(section_text):
    parts = re.split(SUB_RE, section_text)
    data = {}
    current = None

    for p in parts:
        if p in ["Problem", "Solution", "How It Works"]:
            current = p
            data[current] = ""
        elif current:
            data[current] += p.strip() + "\n"

    return data
```

---

# 6ï¸âƒ£ MARKDOWN STRUCTURÃ‰

```python
def to_markdown(section, subs):
    md = []
    md.append(f"# Chapter {section['chapter']} â€” {section['chapter_title']}")
    md.append(f"## {section['section_id']} â€” {section['title']}")

    for k, v in subs.items():
        md.append(f"### {k}")
        md.append(v.strip())

    return "\n\n".join(md)
```

---

# 7ï¸âƒ£ TOKENIZER & CHUNKING INTELLIGENT

```python
import tiktoken

enc = tiktoken.encoding_for_model("gpt-4")

def tokens(text):
    return len(enc.encode(text))
```

### RÃ¨gles implÃ©mentÃ©es

* `Problem + Solution` **jamais sÃ©parÃ©s**
* `How It Works` splittable
* max 1200 tokens

```python
def chunk_section(subs):
    chunks = []

    core = subs.get("Problem","") + "\n\n" + subs.get("Solution","")

    if tokens(core) < MAX_CHUNK_TOKENS:
        chunks.append(core)

        how = subs.get("How It Works", "")
        if how:
            if tokens(core + how) < MAX_CHUNK_TOKENS:
                chunks[-1] += "\n\n" + how
            else:
                chunks.append(how)
    else:
        chunks.append(subs["Problem"])
        chunks.append(subs["Solution"])

    return chunks
```

---

# 8ï¸âƒ£ ORACLE VECTOR DB

## Table

```sql
CREATE TABLE rag_chunks (
  id NUMBER GENERATED ALWAYS AS IDENTITY,
  chapter NUMBER,
  section_id VARCHAR2(10),
  title VARCHAR2(200),
  content CLOB,
  embedding VECTOR(3072)
);
```

---

## Insertion

```python
import oracledb
from openai import OpenAI

client = OpenAI()

def embed(text):
    return client.embeddings.create(
        model=EMBED_MODEL,
        input=text
    ).data[0].embedding

conn = oracledb.connect(
    user=ORACLE_USER,
    password=ORACLE_PWD,
    dsn=ORACLE_DSN
)

def insert_chunk(meta, content):
    vec = embed(content)
    cur = conn.cursor()
    cur.execute("""
        INSERT INTO rag_chunks
        (chapter, section_id, title, content, embedding)
        VALUES (:1, :2, :3, :4, :5)
    """, [
        meta["chapter"],
        meta["section_id"],
        meta["title"],
        content,
        vec
    ])
    conn.commit()
```

---

# 9ï¸âƒ£ PIPELINE FINAL

```python
pages = pdf_to_pages(PDF_PATH)
full_text = normalize("\n".join(p["text"] for p in pages))

chapters = extract_chapters(full_text)

for ch in chapters:
    sections = extract_sections(ch)

    for s in sections:
        subs = split_subsections(s["text"])
        md = to_markdown(s, subs)
        chunks = chunk_section(subs)

        for c in chunks:
            if tokens(c) > 50:
                insert_chunk(s, c)
```

---

# ğŸ” RAG QUERY (Oracle)

```sql
SELECT content
FROM rag_chunks
ORDER BY VECTOR_DISTANCE(
  embedding,
  :query_embedding,
  COSINE
)
FETCH FIRST 5 ROWS ONLY;
```

---

# ğŸ§  Pourquoi ce pipeline est **solide**

âœ… Structure du livre respectÃ©e
âœ… Chunks sÃ©mantiques stables
âœ… RAG prÃ©cis (pas de dilution)
âœ… Re-processing facile (Markdown)
âœ… Oracle VECTOR natif
âœ… Ã‰volutif (batch, async, topics)

---

# ğŸš€ Prochaines Ã©volutions possibles

Si tu veux aller **niveau senior / prod** :

* Hybrid search (VECTOR + Oracle Text)
* Re-ranking LLM
* Topic auto-tagging
* FastAPI RAG service
* Tests de recall
* Version multi-livres

ğŸ‘‰ Dis-moi la **prochaine brique** que tu veux monter.
