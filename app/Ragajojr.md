Excellent ‚úÖ ‚Äî c‚Äôest une **am√©lioration tr√®s intelligente et naturelle** pour un RAG de production.
On va donc faire en sorte que le mod√®le **s‚Äôadapte automatiquement √† la langue de la question** :

* si la question est en fran√ßais ‚Üí r√©ponse en fran√ßais,
* si la question est en anglais ‚Üí r√©ponse en anglais.

Cela se fera via une petite **d√©tection linguistique l√©g√®re** dans `rag.py` avant la g√©n√©ration du prompt.
Je te montre ci-dessous la version finale **am√©lior√©e et compl√®te de `app/rag.py`**, avec cette logique int√©gr√©e proprement üëá

---

## üß† `app/rag.py` (version finale, contextualis√©e + multilingue auto)

```python
# app/rag.py
from datetime import datetime
from typing import List, Dict, Any, Optional
from embedder import embed_texts
from vector_store import get_vanish_collection
from litellm_client import generate_with_litellm  # module √† part
import re


def detect_language(text: str) -> str:
    """
    D√©tection tr√®s simple de la langue (fran√ßais ou anglais) selon les mots.
    """
    text_lower = text.lower()
    fr_score = len(re.findall(r"\b(le|la|les|est|erreur|flux|analyse|probl√®me|cause|action)\b", text_lower))
    en_score = len(re.findall(r"\b(the|is|error|flow|analysis|problem|cause|action)\b", text_lower))
    return "fr" if fr_score >= en_score else "en"


def _build_system_prompt(date_str: str, lang: str) -> str:
    """
    Prompt syst√®me : r√¥le + contexte + date du jour, adapt√© √† la langue d√©tect√©e.
    """
    if lang == "fr":
        return (
            "Tu es un assistant expert en supervision et analyse des pipelines batch (Vanish Flows). "
            "Tu aides les √©quipes op√©rationnelles √† diagnostiquer les √©checs de jobs (SQL, Oracle, Postgres, "
            "VMWare, APEX, etc.), proposer des causes probables et des actions concr√®tes.\n\n"
            f"Date d'analyse : {date_str}\n"
            "R√©ponds en fran√ßais, structur√© avec : Erreurs r√©currentes, Causes probables, Actions recommand√©es, Synth√®se."
        )
    else:
        return (
            "You are an expert assistant specialized in analyzing batch pipelines (Vanish Flows). "
            "You help operations teams diagnose failed jobs (SQL, Oracle, Postgres, VMWare, APEX, etc.), "
            "identify likely causes and propose concrete corrective actions.\n\n"
            f"Analysis date: {date_str}\n"
            "Respond in English, structured with: Recurrent Errors, Probable Causes, Recommended Actions, Summary."
        )


def _build_user_prompt(query: str, contexts: List[Dict[str, Any]], date_str: str, lang: str) -> str:
    """
    Prompt utilisateur enrichi avec la date, les contextes et les consignes selon la langue.
    """
    context_blocks = []
    for i, ctx in enumerate(contexts):
        doc = ctx.get("document", "")
        meta = ctx.get("meta", {})
        id_info = meta.get("_id", "")
        context_blocks.append(f"[Context #{i+1} ‚Äî _id: {id_info}]\n{doc}")

    joined_context = "\n\n---\n\n".join(context_blocks) if context_blocks else "No additional context available."

    if lang == "fr":
        return (
            f"Contrainte : date du rapport = {date_str}\n\n"
            f"QUESTION:\n{query}\n\n"
            f"CONTEXTES:\n{joined_context}\n\n"
            "T√¢che :\n"
            "1Ô∏è‚É£ Liste les erreurs r√©currentes observ√©es.\n"
            "2Ô∏è‚É£ Indique les causes probables (r√©seau, permission, configuration...).\n"
            "3Ô∏è‚É£ Propose des actions concr√®tes et qui contacter.\n"
            "4Ô∏è‚É£ Termine par une synth√®se concise.\n"
            "R√©ponds exclusivement en fran√ßais."
        )
    else:
        return (
            f"Constraint: report date = {date_str}\n\n"
            f"QUESTION:\n{query}\n\n"
            f"CONTEXTS:\n{joined_context}\n\n"
            "Task:\n"
            "1Ô∏è‚É£ List recurrent errors observed.\n"
            "2Ô∏è‚É£ Explain probable causes (network, permission, config...).\n"
            "3Ô∏è‚É£ Suggest concrete actions and who should handle them.\n"
            "4Ô∏è‚É£ End with a concise summary.\n"
            "Answer strictly in English."
        )


def _format_chroma_result(res: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Transforme la r√©ponse Chroma en une liste ordonn√©e de contextes { 'document': str, 'meta': {...} }.
    """
    contexts = []
    try:
        docs = res["documents"][0] if isinstance(res["documents"][0], list) else res["documents"]
        metas = res["metadatas"][0] if isinstance(res["metadatas"][0], list) else res["metadatas"]
    except Exception:
        docs = res.get("documents", [])
        metas = res.get("metadatas", [])

    for i in range(min(len(docs), len(metas))):
        contexts.append({"document": docs[i], "meta": metas[i]})
    return contexts


def answer_query(query: str, top_k: int = 8, date_str: Optional[str] = None) -> Dict[str, Any]:
    """
    RAG complet :
    - D√©tection de la langue du query
    - Recherche vectorielle Chroma
    - G√©n√©ration contextualis√©e via LLM
    """
    if not date_str:
        date_str = datetime.utcnow().strftime("%d/%m/%Y")

    lang = detect_language(query)
    col = get_vanish_collection()
    q_emb = embed_texts([query])[0]

    try:
        res = col.query(query_embeddings=[q_emb], n_results=top_k)
    except Exception as e:
        msg = f"Index vectoriel indisponible : {e}"
        print(msg)
        res = {"documents": [[]], "metadatas": [[]]}

    contexts = _format_chroma_result(res)

    system_prompt = _build_system_prompt(date_str, lang)
    user_prompt = _build_user_prompt(query, contexts, date_str, lang)

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]

    answer_text = generate_with_litellm(messages)

    return {
        "answer": answer_text,
        "contexts": contexts,
        "query": query,
        "lang": lang,
        "date": date_str,
        "timestamp": datetime.utcnow().isoformat(),
    }


# Test rapide (tu peux lancer ce fichier directement)
if __name__ == "__main__":
    print("=== TEST FR ===")
    out_fr = answer_query("Analyse les erreurs critiques du jour et propose des actions.")
    print(out_fr["answer"], "\n")

    print("=== TEST EN ===")
    out_en = answer_query("Analyze today's failed flows and propose corrective actions.")
    print(out_en["answer"])
```

---

### üß© Points cl√©s

| Fonction                 | R√¥le                                               |
| ------------------------ | -------------------------------------------------- |
| `detect_language()`      | D√©tecte automatiquement la langue du prompt        |
| `_build_system_prompt()` | D√©finit le r√¥le et les r√®gles selon la langue      |
| `_build_user_prompt()`   | Construit un prompt d√©taill√© avec date et contexte |
| `answer_query()`         | Orchestration compl√®te du RAG                      |

* Le prompt **ne force plus une seule langue**, mais s‚Äôadapte naturellement.
* Si tu poses une question en anglais, tout le raisonnement et la sortie se feront en anglais.
* Les contextes Chroma restent identiques (car ils sont neutres).

---

Souhaites-tu que je t‚Äôajoute maintenant la version de `litellm_client.py` qui fonctionne directement avec ce RAG (gestion du mod√®le, API key, logs, timeouts, etc.) ?
√áa te permettrait d‚Äôavoir le pipeline complet RAG ‚Üí analyse ‚Üí rapport.
