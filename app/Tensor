Parfait üëç
Tu veux donc que ton **RAG** n‚Äôutilise **aucun mod√®le h√©berg√© sur Hugging Face**, et reste **100 % local**, sans d√©pendre de `huggingface.co`.

üëâ Je te propose d‚Äôutiliser **LaBSE** (Language-agnostic BERT Sentence Embedding), d√©velopp√© par **Google**, t√©l√©chargeable via **TensorFlow Hub** (pas Hugging Face).
C‚Äôest un excellent mod√®le pour produire des embeddings multilingues (fran√ßais inclus üá´üá∑), et il fonctionne enti√®rement **offline** une fois t√©l√©charg√©.

---

## ‚öôÔ∏è √âtapes √† suivre

### 1Ô∏è‚É£ Installe les d√©pendances requises

Ajoute-les √† ton `requirements.txt` :

```txt
tensorflow
tensorflow-hub
```

*(tu peux retirer `sentence-transformers` si tu ne l‚Äôutilises plus)*

---

### 2Ô∏è‚É£ Mets √† jour ton fichier `.env`

```env
LOCAL_EMB_MODEL=https://tfhub.dev/google/LaBSE/2
```

‚ö†Ô∏è Tu peux aussi **t√©l√©charger le mod√®le localement une seule fois** puis l‚Äôutiliser en local (voir plus bas üëá).

---

### 3Ô∏è‚É£ Nouveau fichier `app/embedder.py` (avec LaBSE ‚Äì sans Hugging Face)

```python
import tensorflow_hub as hub
import tensorflow as tf
import numpy as np
import config
import os

_model = None

def get_model():
    global _model
    if _model is None:
        print("üîπ Loading LaBSE model (can take a minute)...")
        # Si le mod√®le est local (ex: ./models/LaBSE)
        if os.path.exists(config.LOCAL_EMB_MODEL):
            _model = hub.load(config.LOCAL_EMB_MODEL)
        else:
            _model = hub.load("https://tfhub.dev/google/LaBSE/2")
    return _model

def embed_texts(texts):
    model = get_model()
    # Normalisation
    def preprocess(sentences):
        return [tf.constant([t]) for t in sentences]
    embeddings = []
    for batch in preprocess(texts):
        emb = model(batch)["default"]
        embeddings.append(np.array(emb[0], dtype=np.float32).tolist())
    return embeddings
```

‚úÖ Ce code :

* Utilise **TensorFlow Hub**, pas Hugging Face.
* T√©l√©charge une fois le mod√®le si tu ne l‚Äôas pas d√©j√† localement.
* Produit des embeddings compatibles avec Chroma (vecteurs `float32`).

---

### 4Ô∏è‚É£ (Optionnel) T√©l√©chargement local une seule fois

Tu peux t√©l√©charger le mod√®le une fois (avec Internet) puis le copier sur ton serveur offline :

```python
import tensorflow_hub as hub
model = hub.load("https://tfhub.dev/google/LaBSE/2")
model.save("./models/LaBSE")
```

Ensuite, dans ton `.env` :

```env
LOCAL_EMB_MODEL=./models/LaBSE
```

Et le backend chargera le mod√®le localement sans aucun acc√®s externe üîí

---

### ‚úÖ R√©sum√©

* **Avantages :**

  * Pas de Hugging Face
  * Compatible avec TensorFlow / CPU
  * Multilingue (fr, en, etc.)
  * Facilement int√©grable √† ton code existant

* **Inconv√©nient :**

  * Un peu plus lourd (~1.2 Go)
  * L‚Äôinf√©rence est un peu plus lente que MiniLM (car mod√®le complet BERT)

---

Souhaites-tu que je te montre **comment adapter `requirements.txt` et `Dockerfile`** pour que tout soit pr√™t √† tourner avec LaBSE (offline) ?
