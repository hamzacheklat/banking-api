Voici une explication claire et concrÃ¨te de **toutes les alertes**, comme si tu Ã©tais en production avec une API rÃ©elle.
Je te dÃ©taille pour chacune :

* ce que Ã§a dÃ©tecte
* pourquoi câ€™est important
* quand Ã§a dÃ©clenche
* quoi faire quand elle sâ€™active

---

# ğŸš¨ Explication des alertes Prometheus

---

# 1) `APIInstanceDown`

```yaml
expr: up == 0
for: 1m
severity: critical
```

## Ce que Ã§a signifie

Prometheus nâ€™arrive plus Ã  contacter lâ€™API.

* Le process est mort
* Le container est crashÃ©
* Le serveur est down
* Le rÃ©seau est cassÃ©

## Quand Ã§a dÃ©clenche

Si lâ€™API ne rÃ©pond plus pendant **1 minute**.

## GravitÃ©

ğŸ”´ **CRITICAL**
Câ€™est une panne directe.

## Action Ã  faire

1. VÃ©rifier le service :

   ```
   systemctl status
   docker ps
   ```
2. VÃ©rifier les logs
3. RedÃ©marrer le service si nÃ©cessaire

---

# 2) `HighErrorRate`

```yaml
expr:
  sum(rate(http_requests_5xx_total[5m]))
  /
  sum(rate(http_requests_total[5m]))
  > 0.05
for: 2m
severity: critical
```

## Ce que Ã§a signifie

Plus de **5% des requÃªtes retournent des erreurs 5xx**.

Exemple :

* 1000 requÃªtes/min
* 60 erreurs/min
  â†’ 6% dâ€™erreurs â†’ alerte

## Quand Ã§a dÃ©clenche

Si le taux dâ€™erreur reste >5% pendant **2 minutes**.

## GravitÃ©

ğŸ”´ **CRITICAL**
Ã‡a veut dire que les utilisateurs subissent des erreurs.

## Causes possibles

* Base de donnÃ©es down
* Service externe cassÃ©
* Bug dÃ©ployÃ© en prod
* Timeout massif

## Action Ã  faire

1. VÃ©rifier logs dâ€™erreurs
2. VÃ©rifier DB / services externes
3. Rollback si nÃ©cessaire

---

# 3) `ErrorSpike`

```yaml
expr: sum(rate(http_requests_5xx_total[1m])) > 5
for: 1m
severity: warning
```

## Ce que Ã§a signifie

Plus de **5 erreurs par seconde**.

## DiffÃ©rence avec HighErrorRate

| Alerte        | Type                  |
| ------------- | --------------------- |
| HighErrorRate | % dâ€™erreurs           |
| ErrorSpike    | volume brut dâ€™erreurs |

## Quand Ã§a dÃ©clenche

Si plus de 5 erreurs/sec pendant 1 minute.

## GravitÃ©

ğŸŸ¡ **WARNING**

## Action

* Surveiller la situation
* VÃ©rifier les logs

---

# 4) `HighLatencyP95`

```yaml
p95 > 1s
for: 3m
severity: warning
```

## Ce que Ã§a signifie

95% des requÃªtes prennent **plus dâ€™1 seconde**.

### Exemple

Sur 100 requÃªtes :

* 95 requÃªtes >1s
* seulement 5 rapides

â†’ expÃ©rience utilisateur dÃ©gradÃ©e

## Quand Ã§a dÃ©clenche

Si p95 >1s pendant 3 minutes.

## GravitÃ©

ğŸŸ¡ **WARNING**

## Causes possibles

* Base de donnÃ©es lente
* CPU saturÃ©
* Trop de requÃªtes
* Service externe lent

## Action

1. Regarder les endpoints lents
2. VÃ©rifier CPU / DB
3. Regarder les requÃªtes les plus longues

---

# 5) `CriticalLatencyP99`

```yaml
p99 > 3s
for: 2m
severity: critical
```

## Ce que Ã§a signifie

1% des requÃªtes prennent plus de **3 secondes**.

Ã‡a veut dire :

* certaines requÃªtes sont **trÃ¨s lentes**
* souvent des timeouts ou blocages

## Quand Ã§a dÃ©clenche

Si p99 >3s pendant 2 minutes.

## GravitÃ©

ğŸ”´ **CRITICAL**

## Causes frÃ©quentes

* Deadlock DB
* API externe en timeout
* Thread bloquÃ©
* Fuite mÃ©moire

## Action

1. Identifier lâ€™endpoint lent
2. VÃ©rifier logs timeout
3. VÃ©rifier ressources

---

# 6) `HighMemoryUsage`

```yaml
process_resident_memory_bytes > 1GB
for: 5m
severity: warning
```

## Ce que Ã§a signifie

Le process utilise plus de **1 Go de RAM**.

## Quand Ã§a dÃ©clenche

Si mÃ©moire >1 Go pendant 5 minutes.

## GravitÃ©

ğŸŸ¡ **WARNING**

## Causes possibles

* Fuite mÃ©moire
* Trop de requÃªtes en parallÃ¨le
* Cache trop gros
* Mauvais usage de la DB

## Action

1. VÃ©rifier lâ€™Ã©volution mÃ©moire dans Grafana
2. RedÃ©marrer le service si nÃ©cessaire
3. Chercher fuite mÃ©moire

---

# 7) `HighCPUUsage`

```yaml
rate(process_cpu_seconds_total[1m]) > 0.8
for: 5m
severity: warning
```

## Ce que Ã§a signifie

Le process utilise **plus de 80% CPU**.

## Quand Ã§a dÃ©clenche

Si CPU >80% pendant 5 minutes.

## GravitÃ©

ğŸŸ¡ **WARNING**

## Causes possibles

* Boucle infinie
* Trop de trafic
* Traitement lourd
* Worker bloquÃ©

## Action

1. VÃ©rifier CPU dans Grafana
2. VÃ©rifier endpoints actifs
3. Augmenter nombre de workers si nÃ©cessaire

---

# RÃ©sumÃ© rapide (tableau)

| Alerte             | Ce que Ã§a veut dire        | GravitÃ©     |
| ------------------ | -------------------------- | ----------- |
| APIInstanceDown    | API inaccessible           | ğŸ”´ Critical |
| HighErrorRate      | >5% erreurs                | ğŸ”´ Critical |
| ErrorSpike         | pic dâ€™erreurs              | ğŸŸ¡ Warning  |
| HighLatencyP95     | API lente pour la majoritÃ© | ğŸŸ¡ Warning  |
| CriticalLatencyP99 | requÃªtes trÃ¨s lentes       | ğŸ”´ Critical |
| HighMemoryUsage    | trop de RAM                | ğŸŸ¡ Warning  |
| HighCPUUsage       | CPU saturÃ©                 | ğŸŸ¡ Warning  |

---

# Ordre de gravitÃ© rÃ©el en production

1. ğŸ”´ APIInstanceDown
2. ğŸ”´ HighErrorRate
3. ğŸ”´ CriticalLatencyP99
4. ğŸŸ¡ HighLatencyP95
5. ğŸŸ¡ ErrorSpike
6. ğŸŸ¡ HighMemoryUsage
7. ğŸŸ¡ HighCPUUsage

---

Si tu veux, je peux te fournir :

* une **stratÃ©gie dâ€™alerting SRE complÃ¨te** (warning vs critical par endpoint)
* des **alerts spÃ©cifiques Ã  ton job Cinego**
* un **dashboard dâ€™investigation** pour diagnostiquer une alerte en 30 secondes.
